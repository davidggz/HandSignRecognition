{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras as keras\n",
    "\n",
    "# Estos tienen solo una sola salida. 24x24\n",
    "df_letras_train = pd.read_csv(\"DatasetsIAO/Letras/sign_mnist_train.csv\")\n",
    "df_letras_test = pd.read_csv(\"DatasetsIAO/Letras/sign_mnist_test.csv\")\n",
    "\n",
    "# Estos están en one hot encoding. 64x64\n",
    "X_numeros = np.load(\"DatasetsIAO/NumerosBien/X.npy\")\n",
    "Y_numeros = np.load(\"DatasetsIAO/NumerosBien/Y.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cambiar Y numeros a solo tener una salida **DONE**\n",
    "2. Juntar test y train de letras y separar en Y y X **DONE**\n",
    "3. Reescalar las imagenes de numeros de 64x64 a 28x28 **DONE**\n",
    "4. Juntar los datos de numeros y los de letras **DONE**\n",
    "5. Pasar todo el test a one hot encoding\n",
    "6. Separar en train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_numeros.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformamos a dataframe el np.array de Numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para manejar los dataset de los números, los pasamos a dataframe\n",
    "\n",
    "# Hacemos el reshape de X_numeros para que solo tenga dos dimensiones\n",
    "# y pueda ser representado en un dataframe.\n",
    "X_numeros = X_numeros.reshape(2062, 4096)\n",
    "\n",
    "# Pasamos tanto Y_numeros como X_numeros a dataframe.\n",
    "X_numeros = pd.DataFrame(data = X_numeros,\n",
    "          index=np.arange(1, X_numeros.shape[0] + 1),\n",
    "          columns=np.arange(1, X_numeros.shape[1] + 1))\n",
    "\n",
    "Y_numeros = pd.DataFrame(data = Y_numeros,\n",
    "          index=np.arange(1, Y_numeros.shape[0] + 1),\n",
    "          columns=np.arange(0, Y_numeros.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasamos de One Hot Encoding a Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos las salidas que están en one hot encoding\n",
    "# Sumamos 26 ya que luego va juntar los dos dataset. El número del 0 al 9 serán los números del 26 al 35\n",
    "Y_numeros['Clase'] = Y_numeros.apply(lambda row: np.argmax(row.values) + 26, axis = 1)\n",
    "\n",
    "# Eliminamos las columnas en one hot encoding\n",
    "Y_numeros = Y_numeros.drop([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], axis = 1)\n",
    "# Renombramos la clase para poder concatenarla posteriormente.\n",
    "Y_numeros = Y_numeros.rename(columns={'Clase': 0})\n",
    "Y_numeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenación de train y test de letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenamos los dataframes de las letras (test y train)\n",
    "dataframes = [df_letras_train, df_letras_test]\n",
    "\n",
    "# Ponemos ignore_index para que se vuelva a hacer el index desde cero.\n",
    "dataset_letras = pd.concat(dataframes, ignore_index = True)\n",
    "dataset_letras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación de letras en X e Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataset de las letras en Y y X (labels y data)\n",
    "Y_letras = dataset_letras['label']\n",
    "#Y_letras = Y_letras.rename(columns={\"label\": \"Clase\"})\n",
    "X_letras = dataset_letras.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_letras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize de las imagenes de Numeros a 24x24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenesNumeros = X_numeros.to_numpy().reshape(-1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "newX = []\n",
    "# Hacemos reshape de todas las imagenes\n",
    "for img in imagenesNumeros:\n",
    "    newX.append(cv2.resize(img, dsize=(28, 28), interpolation = cv2.INTER_CUBIC))\n",
    "\n",
    "# Hacemos reshape de las imágenes de nuevo para poder hacer el dataframe\n",
    "X_numeros = np.asarray(newX).reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a transformar X_numeros en un dataframe para concatenarlo\n",
    "X_numeros = pd.DataFrame(data = X_numeros,\n",
    "          index=np.arange(1, X_numeros.shape[0] + 1),\n",
    "          columns=np.arange(1, X_numeros.shape[1] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos el dataframe de letras, ya que el dataframe de números \n",
    "# estaba normalizado y lo vamos a necesitar para entrenar a la red\n",
    "# de neuronas correctamente.\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x = X_letras.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "# Debemos pasarle el nombre de las nuevas columnas ya que si no\n",
    "# no se concatenan correctamente.\n",
    "X_letras = pd.DataFrame(x_scaled, columns=list(range(1,785)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Hacemos reshape para que pasen de tener una sola dimensión, a tener 2 (de 784 a 28x28).\n",
    "# Esto es necesario para imprimir la imagen.\n",
    "prueba2 = X_letras.to_numpy().reshape(-1, 28, 28)\n",
    "print(prueba2.shape)\n",
    "plt.imshow(prueba2[1000], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "prueba2 = X_numeros.to_numpy().reshape(-1, 28, 28)\n",
    "print(prueba2.shape)\n",
    "plt.imshow(prueba2[750], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntamos los dos dataset y randomizamos las instancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_datasetTotal = [X_letras, X_numeros]\n",
    "Y_datasetTotal = [Y_letras, Y_numeros]\n",
    "\n",
    "X_datasetTotal = pd.concat(X_datasetTotal, ignore_index = True)\n",
    "Y_datasetTotal = pd.concat(Y_datasetTotal, ignore_index = True)\n",
    "Y_datasetTotal = Y_datasetTotal.rename(columns={0: 'Clase'})\n",
    "Y_datasetTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cada vez que se ejecute esta celda, se obtendrá una aleatorización distinta de los datos.\n",
    "\n",
    "# Concatenamos las columnas para poder manejarlas mejor al hacer shuffle\n",
    "result = pd.concat([X_datasetTotal, Y_datasetTotal], axis = 1)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "# Aleatorizamos las imagenes\n",
    "result = shuffle(result)\n",
    "\n",
    "# Volvemos a generar los conjuntos X e Y\n",
    "X_datasetTotal = result.drop('Clase', axis = 1)\n",
    "Y_datasetTotal = result['Clase']\n",
    "\n",
    "X_datasetTotal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparamos las imágenes y las pasamos a One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el reshape para que las imagenes tengan el tamaño correcto\n",
    "X_datasetTotal = X_datasetTotal.values.reshape(X_datasetTotal.shape[0], 28, 28, 1)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Y_datasetTotal = to_categorical(Y_datasetTotal.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_datasetTotal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay un total de 36 columnas por los siguientes motivos:\n",
    "\n",
    "- En total, el alfabeto tiene 26 letras. Hay dos que no podemos clasificar: la j y la z. La z no supone un problema ya que se encuentra al final, por lo que directamente no se tiene en cuenta. La j, al estar en el medio, el dataset se salta la etiqueta que debería tener. Esto hace que aun teniendo 24 etiquetas posibles, el array generado al hacer One Hot Encoding, llegue hasta la posición 25.\n",
    "\n",
    "\n",
    "- Al elegir las nuevas etiquetas de los números, le sumamos 26 debido a que la última etiqueta de las letras es la 25 (por los motivos recién mencionados). Esto hace que 26 + 10 = 36 que es el número de posibles valores que tenemos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separamos en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "nInstancias = math.ceil(0.7 * X_datasetTotal.shape[0])\n",
    "\n",
    "X_train = X_datasetTotal[:nInstancias]\n",
    "X_test = X_datasetTotal[nInstancias + 1:]\n",
    "\n",
    "Y_train = Y_datasetTotal[:nInstancias]\n",
    "Y_test = Y_datasetTotal[nInstancias + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Modelo de red neuronal\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(36, activation='softmax'))\n",
    "          \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, Y_train, batch_size=32)\n",
    "test_generator = test_datagen.flow(X_test, Y_test, batch_size=32)\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizers.RMSprop(lr=1e-4), \n",
    "              metrics=['categorical_crossentropy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=X_train.shape[0]//32,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=X_test.shape[0]//32\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modelo.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosas antiguas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el reshape para que las imagenes tengan el tamaño correcto\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "# Pasamos las salidas a one hot encoding. Aunque tenemos 24 clases\n",
    "# obtenemos un array de 25 posiciones. Esto se debe a que la J (numero\n",
    "# 9) también se tiene en cuenta, aunque no aparezca en ningún momento.\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# print(X_train.shape)\n",
    "# plt.imshow(X_train[0][:][:], cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(path_npy + \"y_mnist_train.npy\", y)\n",
    "# np.save(path_npy + \"X_mnist_train.npy\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "# y = keras.utils.to_categorical(y)\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Modelo de red neuronal\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(25, activation='softmax'))\n",
    "          \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "test_generator = test_datagen.flow(X_test, y_test, batch_size=32)\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizers.RMSprop(lr=1e-4), \n",
    "              metrics=['categorical_crossentropy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='log/',\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=X_train.shape[0]//32,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=X_test.shape[0]//32,\n",
    "    callbacks=callbacks\n",
    "    )\n",
    "\n",
    "# model.save(\"modelo.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
